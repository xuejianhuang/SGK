2024-11-01 09:51:10,352 - ============================== args ==============================
2024-11-01 09:51:10,353 - dataset: weibo
2024-11-01 09:51:10,353 - model: SGKE
2024-11-01 09:51:10,353 - batch: 32
2024-11-01 09:51:10,353 - seed: 666
2024-11-01 09:51:10,353 - ============================== End args ==============================
2024-11-01 09:51:10,353 - ============================== config ==============================
2024-11-01 09:51:10,353 - BertModel: <class 'transformers.models.bert.modeling_bert.BertModel'>
2024-11-01 09:51:10,353 - BertTokenizer: <class 'transformers.models.bert.tokenization_bert.BertTokenizer'>
2024-11-01 09:51:10,353 - att_dropout: 0
2024-11-01 09:51:10,353 - att_num_heads: 8
2024-11-01 09:51:10,353 - batch_size: 32
2024-11-01 09:51:10,353 - bert_dir: ./bert-base-multilingual-uncased/
2024-11-01 09:51:10,353 - bert_freeze: False
2024-11-01 09:51:10,353 - classifier_hidden_dim: 128
2024-11-01 09:51:10,353 - decayRate: 0.96
2024-11-01 09:51:10,353 - device: cuda
2024-11-01 09:51:10,353 - edge_feats: 768
2024-11-01 09:51:10,353 - epoch: 10
2024-11-01 09:51:10,353 - f_dropout: 0
2024-11-01 09:51:10,353 - hidden_dim: 768
2024-11-01 09:51:10,353 - img_dim: 768
2024-11-01 09:51:10,353 - knowledge_enhanced: True
2024-11-01 09:51:10,353 - lr: 5e-05
2024-11-01 09:51:10,353 - max_captions_num: 5
2024-11-01 09:51:10,353 - max_images_num: 5
2024-11-01 09:51:10,353 - model_saved_path: ./best_model/
2024-11-01 09:51:10,353 - n_layers: 2
2024-11-01 09:51:10,354 - node_feats: 768
2024-11-01 09:51:10,354 - num_classes: 3
2024-11-01 09:51:10,354 - num_heads: 2
2024-11-01 09:51:10,354 - out_feats: 768
2024-11-01 09:51:10,354 - patience: 3
2024-11-01 09:51:10,354 - resnet101_path: ./ResNet/resnet101-5d3b4d8f.pth
2024-11-01 09:51:10,354 - resnet50_path: ./ResNet/resnet50-11ad3fa6.pth
2024-11-01 09:51:10,354 - swin_transformer: ./swin-transformer
2024-11-01 09:51:10,354 - test_ratio: 0.1
2024-11-01 09:51:10,354 - text_dim: 768
2024-11-01 09:51:10,354 - text_max_length: 40
2024-11-01 09:51:10,354 - torch: <module 'torch' from '/home/huang001/anaconda3/lib/python3.11/site-packages/torch/__init__.py'>
2024-11-01 09:51:10,354 - train_ratio: 0.8
2024-11-01 09:51:10,354 - twitter_dataset_dir: ./data/Twitter/
2024-11-01 09:51:10,354 - val_ratio: 0.1
2024-11-01 09:51:10,354 - weibo_dataset_dir: ./data/Weibo/
2024-11-01 09:51:10,354 - ============================== End config ==============================
2024-11-01 09:51:17,613 - total number of parameters:240711671
2024-11-01 09:51:17,617 - -----------Epoch:0-----------
2024-11-01 10:16:43,490 - train_loss:0.42788 train_acc:0.804
2024-11-01 10:19:24,332 - val_loss:0.25987 val_acc:0.879 

2024-11-01 10:19:25,775 - save model,acc:0.879
2024-11-01 10:19:25,775 - -----------Epoch:1-----------
2024-11-01 10:45:08,331 - train_loss:0.20885 train_acc:0.914
2024-11-01 10:47:48,010 - val_loss:0.19006 val_acc:0.920 

2024-11-01 10:47:49,693 - save model,acc:0.920
2024-11-01 10:47:49,693 - -----------Epoch:2-----------
2024-11-01 11:13:23,540 - train_loss:0.09997 train_acc:0.960
2024-11-01 11:16:02,432 - val_loss:0.18283 val_acc:0.929 

2024-11-01 11:16:04,163 - save model,acc:0.929
2024-11-01 11:16:04,164 - -----------Epoch:3-----------
2024-11-01 11:41:32,469 - train_loss:0.05388 train_acc:0.980
2024-11-01 11:44:10,689 - val_loss:0.24231 val_acc:0.920 

2024-11-01 11:44:10,690 - -----------Epoch:4-----------
2024-11-01 12:09:41,280 - train_loss:0.02909 train_acc:0.987
2024-11-01 12:12:19,603 - val_loss:0.25607 val_acc:0.928 

2024-11-01 12:12:19,604 - -----------Epoch:5-----------
2024-11-01 12:37:55,013 - train_loss:0.01556 train_acc:0.993
2024-11-01 12:40:33,602 - val_loss:0.26007 val_acc:0.927 

2024-11-01 12:43:09,088 - --------------------- test results-------------------------------
2024-11-01 12:43:09,089 - acc:0.92416036  prec:[0.94052047 0.90909094 0.9325153 ]  rec:[0.96564883 0.87431693 0.9325153 ]  f1:[0.952919   0.89136493 0.9325153 ]
2024-11-01 12:43:09,089 - confusion: 
[[253   3   6]
 [  7 160  16]
 [  9  13 304]]
2024-11-01 12:43:09,151 - the running time is: 10311.5 s
